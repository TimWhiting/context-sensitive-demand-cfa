##############################################
## Entries needing conversion to bib-scrape ##
##############################################

@article{palsberg1995closure,
 author = {Palsberg, Jens},
 title = {Closure Analysis in Constraint Form},
 year = {1995},
 issue_date = {Jan. 1995},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {17},
 number = {1},
 issn = {0164-0925},
 url = {https://doi.org/10.1145/200994.201001},
 doi = {10.1145/200994.201001},
 journal = {ACM Trans. Program. Lang. Syst.},
 month = jan,
 pages = {47--62},
 numpages = {16},
 keywords = {flow analysis, correctness proof, constraints}
}

@inproceedings{smaragdakis2018defensive,
  title={Defensive Points-To Analysis: Effective Soundness via Laziness},
  author={Smaragdakis, Yannis and Kastrinis, George},
  booktitle={32nd European Conference on Object-Oriented Programming (ECOOP 2018)},
  year={2018},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@techreport{plt-tr1,
  title       = {Reference: Racket},
  author      = {Matthew Flatt and PLT},
  number      = {PLT-TR-2010-1},
  institution = {PLT Design Inc.},
  year        = {2010},
  note        = {\url{https://racket-lang.org/tr1/}}
}

@article{facchinetti2019ddpa,
 author = {Facchinetti, Leandro and Palmer, Zachary and Smith, Scott},
 title = {Higher-Order Demand-Driven Program Analysis},
 year = {2019},
 issue_date = {July 2019},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {41},
 number = {3},
 issn = {0164-0925},
 url = {https://doi.org/10.1145/3310340},
 doi = {10.1145/3310340},
 journal = {ACM Trans. Program. Lang. Syst.},
 month = jul,
 articleno = {14},
 numpages = {53},
 keywords = {context-sensitive, Functional programming, polynomial-time, flow-sensitive, demand-driven, program analysis, pushdown system} }

@inproceedings{bravenboer2009strictly,
 author = {Bravenboer, Martin and Smaragdakis, Yannis},
 title = {Strictly Declarative Specification of Sophisticated Points-to Analyses},
 booktitle = {Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications},
 series = {OOPSLA '09},
 year = {2009},
 isbn = {978-1-60558-766-0},
 location = {Orlando, Florida, USA},
 pages = {243--262},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/1640089.1640108},
 doi = {10.1145/1640089.1640108},
 acmid = {1640108},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DOOP, bdds, datalog, declarative, points-to analysis},
} 

@article{horwitz1995demand,
  title={Demand interprocedural dataflow analysis},
  author={Horwitz, Susan and Reps, Thomas and Sagiv, Mooly},
  journal={ACM SIGSOFT Software Engineering Notes},
  volume={20},
  number={4},
  pages={104--115},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@article{li2020principled,
  title={A Principled Approach to Selective Context Sensitivity for Pointer Analysis},
  author={Li, Yue and Tan, Tian and M{\o}ller, Anders and Smaragdakis, Yannis},
  journal={ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume={42},
  number={2},
  pages={1--40},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@phdthesis{darais2017diss,
  author = {Darais, David Charles},
  title = {Mechanizing Abstract Interpretation},
  school = {University of Maryland},
  address = {College Park, MD, USA},
  year = {2017}
}


###########################################
## Entries that bib-scrape cannot handle ##
###########################################

@phdthesis{dvanhorn:Shivers:1991:CFA,
  author = {Shivers, Olin},
  publisher = {Carnegie Mellon University},
  school = {Carnegie Mellon University},
  title = {Control-Flow Analysis of Higher-Order Languages},
  address = {Pittsburgh, PA, USA},
  year = {1991}
}

@book{dvanhorn:Neilson:1999,
  author = {Nielson, Flemming and Nielson, Hanne R. and Hankin, Chris},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  title = {Principles of Program Analysis},
  isbn = {3-540-65410-0},
  year = {1999}
}

@phdthesis{local:vardoulakis-diss12,
  author = {Dimitrios Vardoulakis},
  title = {{CFA2: Pushdown Flow Analysis for Higher-Order Languages}},
  school = {Northeastern University},
  address = {Boston, MA, USA},
  year = {2012}
}

@other{local:cousot-bib,
  author = {Patrick Cousot},
  title = {Abstract Interpretation},
  x-url = {http://www.di.ens.fr/~cousot/AI/}
  }

@inproceedings{su2014parallel,
 author = {Su, Yu and Ye, Ding and Xue, Jingling},
 title = {Parallel Pointer Analysis with {CFL}-Reachability},
 year = {2014},
 isbn = {978-1-4799-5618-0},
 pages = {451--460},
 numpages = {10},
 doi = {10.1109/ICPP.2014.54},
 booktitle = {43rd International Conference on Parallel Processing ({ICPP} 2014)},
 publisher = {{IEEE} Computer Society},
 address = {Los Alamitos, CA, USA},
}

@inproceedings{local:gluck-schmidtfest13,
  author    = "Gl√ºck, Robert",
  year      = "2013",
  title     = "Simulation of Two-Way Pushdown Automata Revisited",
  editor    = "Banerjee, Anindya and Danvy, Olivier and Doh, Kyung-Goo and Hatcliff, John",
  booktitle = "Semantics, Abstract Interpretation, and Reasoning about Programs: Essays Dedicated to David A. Schmidt on the Occasion of his Sixtieth Birthday,
               Manhattan, Kansas, USA, 19-20th September 2013",
  series    = "Electronic Proceedings in Theoretical Computer Science",
  x-volume    = "129",
  publisher = "Open Publishing Association",
  xpages     = "250-258",
  x-doi       = "10.4204/EPTCS.129.15",
}

@inproceedings{palmer2016higher,
  author = {Palmer, Zachary and Smith, Scott F.},
  title = {Higher-Order Demand-Driven Program Analysis},
  booktitle = {30th European Conference on Object-Oriented Programming (ECOOP 2016)},
  year = {2016},

  pages = {19:1--19:25},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN = {978-3-95977-014-9},
  ISSN = {1868-8969},
  volume = {56},
  editor = {Shriram Krishnamurthi and Benjamin S. Lerner},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  address = {Dagstuhl, Germany},
  doi = {10.4230/LIPIcs.ECOOP.2016.19},
}

@inproceedings{spath2016boomerang,
  author    = {Johannes Sp{\"{a}}th and Lisa Nguyen Quang Do and Karim Ali and Eric Bodden},
  editor    = {Shriram Krishnamurthi and Benjamin S. Lerner},
  title     = {{B}oomerang: Demand-Driven Flow- and Context-Sensitive Pointer Analysis for {J}ava},
  booktitle = {30th European Conference on Object-Oriented Programming, {ECOOP} 2016, July 18-22, 2016, Rome, Italy},
  ISBN      = {978-3-95977-014-9},
  ISSN      = {1868-8969},
  series    = {Leibniz International Proceedings in Informatics (LIPIcs)},
  volume    = {56},
  pages     = {22:1--22:26},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik},
  address   = {Dagstuhl, Germany},
  year      = {2016},
  doi       = {10.4230/LIPIcs.ECOOP.2016.22},
}

@article{dvanhorn:Vardoulakis2011CFA2,
  author    = {Dimitrios Vardoulakis and Olin Shivers},
  title     = {{CFA2:} a Context-Free Approach to Control-Flow Analysis},
  journal   = {Logical Methods in Computer Science},
  volume    = {7},
  number    = {2},
  year      = {2011},
  doi       = {10.2168/LMCS-7(2:3)2011},
  numpages  = {39},
}

#####################################
## Entries generated by bib-scrape ##
#####################################

@inproceedings{sui2016supa,
  author = {Sui, Yulei and Xue, Jingling},
  title = {On-demand strong update analysis via value-flow refinement},
  booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  series = {FSE 2016},
  location = {Seattle, WA, USA},
  pages = {460--473},
  numpages = {14},
  month = nov,
  year = {2016},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-4218-6},
  doi = {10.1145/2950290.2950296},
  acmid = {2950296},
  bib_scrape_url = {http://doi.acm.org/10.1145/2950290.2950296},
  keywords = {flow sensitivity; pointer analysis; strong updates; value flow},
  abstract = {We present a new Strong UPdate Analysis for C programs, called Supa, that enables computing points-to information on-demand via value-flow refinement, in environments with small time and memory budgets such as IDEs. We formulate Supa by solving a graph-reachability problem on a value- flow graph representation of the program, so that strong updates are performed where needed, as long as the total analysis budget is not exhausted. Supa facilitates efficiency and precision tradeoffs by allowing different pointer analyses to be applied in a hybrid multi-stage analysis framework. 
{\par}
We have implemented Supa in LLVM with its artifact available at [1]. We evaluate Supa by choosing uninitialized pointer detection as a major client on 12 open-source C programs. As the analysis budget increases, Supa achieves improved precision, with its single-stage flow-sensitive analysis reaching 97{\%} of that achieved by whole-program flow- sensitive analysis by consuming about 0.19 seconds and 36KB of memory per query, on average (with a budget of at most 10000 value-flow edges per query).},
}

@inproceedings{saha2005incremental,
  author = {Saha, Diptikalyan and Ramakrishnan, C. R.},
  title = {Incremental and demand-driven points-to analysis using logic programming},
  booktitle = {Proceedings of the 7th ACM SIGPLAN International Conference on Principles and Practice of Declarative Programming},
  series = {PPDP~'05},
  location = {Lisbon, Portugal},
  pages = {117--128},
  numpages = {12},
  month = jul,
  year = {2005},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {1-59593-090-6},
  doi = {10.1145/1069774.1069785},
  acmid = {1069785},
  bib_scrape_url = {http://doi.acm.org/10.1145/1069774.1069785},
  keywords = {demand-drive analysis; incremental analysis; logic programming; pointer analysis},
  abstract = {Several program analysis problems can be cast elegantly as a logic program. In this paper we show how recently-developed techniques for incremental evaluation of logic programs can be refined and used for deriving practical implementations of incremental program analyzers. Incremental program analyzers compute the changes to the analysis information due to small changes in the input program rather than re-analyzing the program. Demand-driven analyzers compute only the information requested by the client analysis/optimization. We describe a framework based on logic programming for implementing program analyses that combines incremental and demand driven techniques. We show the effectiveness of this approach by building a practical incremental and demand-driven context insensitive points-to analysis and evaluating this implementation for analyzing C programs with 10-70K lines of code. Experiments show that our technique can compute the changes to analysis information due to small changes in the input program in, on the average, 6{\%} of the time it takes to reanalyze the program from scratch, and with little space overhead.},
}

@inproceedings{lu2013incremental,
  author = {Lu, Yi and Shang, Lei and Xie, Xinwei and Xue, Jingling},
  editor = {Jhala, Ranjit and De Bosschere, Koen},
  affiliation = {University of New South Wales and University of New South Wales and University of New South Wales and National University of Defence Technology and University of New South Wales},
  title = {An Incremental Points-to Analysis with {CFL}-Reachability},
  booktitle = {Compiler Construction},
  volume = {7791},
  series = {Lecture Notes in Computer Science},
  pages = {61--81},
  year = {2013},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  language = {English},
  isbn = {978-3-642-37050-2 (Print) 978-3-642-37051-9 (Online)},
  issn = {0302-9743 (Print) 1611-3349 (Online)},
  doi = {10.1007/978-3-642-37051-9_4},
  bib_scrape_url = {https://link.springer.com/chapter/10.1007%2F978-3-642-37051-9_4},
  abstract = {Developing scalable and precise points-to analyses is increasingly important for analysing and optimising object-oriented programs where pointers are used pervasively. An incremental analysis for a program updates the existing analysis information after program changes to avoid reanalysing it from scratch. This can be efficiently deployed in software development environments where code changes are often small and frequent. This paper presents an incremental approach for demand-driven context-sensitive points-to analyses based on Context-Free Language (CFL) reachability. By tracing the CFL-reachable paths traversed in computing points-to sets, we can precisely identify and recompute on demand only the points-to sets affected by the program changes made. Combined with a flexible policy for controlling the granularity of traces, our analysis achieves significant speedups with little space overhead over reanalysis from scratch when evaluated with a null dereferencing client using 14 Java benchmarks.},
}

@inproceedings{shang2012demand,
  author = {Shang, Lei and Xie, Xinwei and Xue, Jingling},
  title = {On-demand dynamic summary-based points-to analysis},
  booktitle = {Proceedings of the Tenth International Symposium on Code Generation and Optimization},
  series = {CGO~'12},
  location = {San Jose, California},
  pages = {264--274},
  numpages = {11},
  month = mar,
  year = {2012},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-1206-6},
  doi = {10.1145/2259016.2259050},
  acmid = {2259050},
  bib_scrape_url = {http://doi.acm.org/10.1145/2259016.2259050},
  keywords = {CFL reachability; demand-driven analysis; dynamic summary; points-to analysis},
  abstract = {Static analyses can be typically accelerated by reducing redundancies. Modern demand-driven points-to or alias analysis techniques rest on the foundation of Context-Free Language (CFL) reachability. These techniques achieve high precision efficiently for a small number of queries raised in small programs but may still be too slow in answering many queries for large programs in a context-sensitive manner.
{\par}
We present an approach, called DynSum, to perform context-sensitive demand-driven points-to analysis fully on-demand by means of computing CFL-reachability summaries without any precision loss. The novelty lies in initially performing a \textit{Partial Points-To Analysis} (PPTA) within a method, which is field-sensitive but context-independent, to summarize its local points-to relations encountered during a query and reusing this information later in the same or different calling contexts. We have compared DynSum with RefinePTS, a refinement-based analysis, using three clients (safe casting, null dereferencing and factory methods) for a suite of nine Java programs. DynSum's average speedups are 1.95x, 2.28x and 1.37x, respectively. We have also compared DynSum with a static approach, which is referred to StaSum here, to show its improved scalability for the same three clients.},
}

@inproceedings{Amin:2017:10.1145/3009837.3009866,
  author = {Amin, Nada and Rompf, Tiark},
  title = {Type soundness proofs with definitional interpreters},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  location = {Paris, France},
  pages = {666--679},
  numpages = {14},
  month = jan,
  year = {2017},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-4660-3},
  doi = {10.1145/3009837.3009866},
  acmid = {3009866},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?id=3009866},
  keywords = {DOT; Definitional interpreters; Scala; dependent object types; type soundness},
  abstract = {While type soundness proofs are taught in every graduate PL class, the gap between realistic languages and what is accessible to formal proofs is large. In the case of Scala, it has been shown that its formal model, the Dependent Object Types (DOT) calculus, cannot simultaneously support key metatheoretic properties such as environment narrowing and subtyping transitivity, which are usually required for a type soundness proof. Moreover, Scala and many other realistic languages lack a general substitution property. 
{\par}
The first contribution of this paper is to demonstrate how type soundness proofs for advanced, polymorphic, type systems can be carried out with an operational semantics based on high-level, definitional interpreters, implemented in Coq. We present the first mechanized soundness proofs in this style for System F and several extensions, including mutable references. Our proofs use only straightforward induction, which is significant, as the combination of big-step semantics, mutable references, and polymorphism is commonly believed to require coinductive proof techniques. 
{\par}
The second main contribution of this paper is to show how DOT-like calculi emerge from straightforward generalizations of the operational aspects of F, exposing a rich design space of calculi with path-dependent types inbetween System F and DOT, which we dub the System D Square. 
{\par}
By working directly on the target language, definitional interpreters can focus the design space and expose the invariants that actually matter at runtime. Looking at such runtime invariants is an exciting new avenue for type system design.},
}

@article{johnson:earl:dvanhorn:PushdownGarbage, 
  title={Pushdown flow analysis with abstract garbage collection}, 
  volume={24}, 
  DOI={10.1017/S0956796814000100}, 
  number={2‚Äì3}, 
  journal={Journal of Functional Programming}, 
  author={JOHNSON, J. IAN and SERGEY, ILYA and EARL, CHRISTOPHER and MIGHT, MATTHEW and VAN HORN, DAVID}, 
  year={2014}, 
  pages={218‚Äì283}}

@inproceedings{dvanhorn:Earl2012Pushdown,
  author = {Earl, Christopher and Sergey, Ilya and Might, Matthew and Van Horn, David},
  title = {Introspective pushdown analysis of higher-order programs},
  booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP~'12},
  location = {Copenhagen, Denmark},
  pages = {177--188},
  numpages = {12},
  month = sep,
  year = {2012},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-1054-3},
  doi = {10.1145/2364527.2364576},
  acmid = {2364576},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=2364527.2364576},
  keywords = {abstract garbage collection; abstract interpretation; abstract machines; cfa2; higher-order languages; program analysis; pushdown analysis; pushdown systems},
  abstract = {In the static analysis of functional programs, pushdown flow analysis and abstract garbage collection skirt just inside the boundaries of soundness and decidability. Alone, each method reduces analysis times and boosts precision by orders of magnitude. This work illuminates and conquers the theoretical challenges that stand in the way of combining the power of these techniques. The challenge in marrying these techniques is not subtle: computing the reachable control states of a pushdown system relies on limiting access during transition to the top of the stack; abstract garbage collection, on the other hand, needs full access to the entire stack to compute a root set, just as concrete collection does. \textit{Introspective} pushdown systems resolve this conflict. Introspective pushdown systems provide enough access to the stack to allow abstract garbage collection, but they remain restricted enough to compute control-state reachability, thereby enabling the sound and precise product of pushdown analysis and abstract garbage collection. Experiments reveal synergistic interplay between the techniques, and the fusion demonstrates {\textquotedbl}better-than-both-worlds{\textquotedbl} precision.},
}

@inproceedings{dvanhorn:Johnson2014Abstracting,
  author = {Johnson, James Ian and Van Horn, David},
  title = {Abstracting abstract control},
  booktitle = {Proceedings of the 10th ACM Symposium on Dynamic Languages},
  series = {DLS~'14},
  location = {Portland, Oregon, USA},
  pages = {11--22},
  numpages = {12},
  month = oct,
  year = {2014},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-3211-8},
  doi = {10.1145/2661088.2661098},
  acmid = {2661098},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=2661088.2661098},
  keywords = {abstract interpretation; abstract machines; continuation marks; control-flow analysis; delimited continuations; pushdown analysis},
  abstract = {The strength of a dynamic language is also its weakness: run-time flexibility comes at the cost of compile-time predictability. Many of the hallmarks of dynamic languages such as closures, continuations, various forms of reflection, and a lack of static types make many programmers rejoice, while compiler writers, tool developers, and verification engineers lament. The dynamism of these features simply confounds statically reasoning about programs that use them. Consequently, static analyses for dynamic languages are few, far between, and seldom sound.
{\par}
The {\textquotedbl}abstracting abstract machines{\textquotedbl} (AAM) approach to constructing static analyses has recently been proposed as a method to ameliorate the difficulty of designing analyses for such language features. The approach, so called because it derives a function for the sound and computable approximation of program behavior starting from the abstract machine semantics of a language, provides a viable approach to dynamic language analysis since all that is required is a machine description of the interpreter.
{\par}
The AAM recipe as originally described produces finite state abstractions: the behavior of a program is approximated as a finite state machine. Such a model is inherently imprecise when it comes to reasoning about the control stack of the interpreter: a finite state machine cannot faithfully represent a stack. Recent advances have shown that higher-order programs can be approximated with pushdown systems. However, such models, founded in automata theory, either breakdown or require significant engineering in the face of dynamic language features that inspect or modify the control stack.
{\par}
In this paper, we tackle the problem of bringing pushdown flow analysis to the domain of dynamic language features. We revise the abstracting abstract machines technique to target the stronger computational model of pushdown systems. In place of automata theory, we use only abstract machines and memoization. As case studies, we show the technique applies to a language with closures, garbage collection, stack-inspection, and first-class composable continuations.},
}

@article{wei2018refunctionalization,
  author = {Wei, Guannan and Decker, James and Rompf, Tiark},
  title = {Refunctionalization of abstract abstract machines: bridging the gap between abstract abstract machines and abstract definitional interpreters (functional pearl)},
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {ICFP},
  pages = {105:1--105:28},
  articleno = {105},
  numpages = {28},
  month = jul,
  year = {2018},
  issue_date = {September 2018},
  publisher = {ACM},
  address = {New York, NY, USA},
  issn = {2475-1421},
  doi = {10.1145/3236800},
  acmid = {3236800},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=3243631.3236800},
  keywords = {Scala; abstract machines; control-flow analysis; refunctionalization},
  abstract = {Abstracting abstract machines is a systematic methodology for constructing sound static analyses for higher-order languages, by deriving small-step abstract abstract machines (AAMs) that perform abstract interpretation from abstract machines that perform concrete evaluation. Darais et al. apply the same underlying idea to monadic definitional interpreters, and obtain monadic abstract definitional interpreters (ADIs) that perform abstract interpretation in big-step style using monads. Yet, the relation between small-step abstract abstract machines and big-step abstract definitional interpreters is not well studied. 
{\par}
In this paper, we explain their functional correspondence and demonstrate how to systematically transform small-step abstract abstract machines into big-step abstract definitional interpreters. Building on known semantic interderivation techniques from the concrete evaluation setting, the transformations include linearization, lightweight fusion, disentanglement, refunctionalization, and the left inverse of the CPS transform. Linearization expresses nondeterministic choice through first-order data types, after which refunctionalization transforms the first-order data types that represent continuations into higher-order functions. The refunctionalized AAM is an abstract interpreter written in continuation-passing style (CPS) with two layers of continuations, which can be converted back to direct style with delimited control operators. Based on the known correspondence between delimited control and monads, we demonstrate that the explicit use of monads in abstract definitional interpreters is optional. 
{\par}
All transformations properly handle the collecting semantics and nondeterminism of abstract interpretation. Remarkably, we reveal how precise call/return matching in control-flow analysis can be obtained by refunctionalizing a small-step abstract abstract machine with proper caching.},
}

@inproceedings{schoepe2023lifting,
  title={Lifting on-demand analysis to higher-order languages},
  author={Schoepe, Daniel and Seekatz, David and Stoilkovska, Ilina and Stucki, Sandro and Tattersall, Daniel and Bolignano, Pauline and Raimondi, Franco and Chang, Bor-Yuh Evan},
  booktitle={International Static Analysis Symposium},
  pages={460--484},
  year={2023},
  organization={Springer}
}

@article{darais2017abstracting,
  author = {Darais, David and Labich, Nicholas and Nguyen, Ph{\'u}c C. and Van Horn, David},
  title = {Abstracting definitional interpreters (functional pearl)},
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {1},
  number = {ICFP},
  pages = {12:1--12:25},
  articleno = {12},
  numpages = {25},
  month = aug,
  year = {2017},
  issue_date = {September 2017},
  publisher = {ACM},
  address = {New York, NY, USA},
  issn = {2475-1421},
  doi = {10.1145/3110256},
  acmid = {3110256},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=3136534.3110256},
  keywords = {abstract interpreters; interpreters},
  abstract = {In this functional pearl, we examine the use of definitional interpreters as a basis for abstract interpretation of higher-order programming languages. As it turns out, definitional interpreters, especially those written in monadic style, can provide a nice basis for a wide variety of collecting semantics, abstract interpretations, symbolic executions, and their intermixings. 
{\par}
But the real insight of this story is a replaying of an insight from Reynold's landmark paper, \emph{Definitional Interpreters for Higher-Order Programming Languages}, in which he observes definitional interpreters enable the defined-language to inherit properties of the defining-language. We show the same holds true for definitional \emph{abstract} interpreters. Remarkably, we observe that abstract definitional interpreters can inherit the so-called {\textquotedblleft}pushdown control flow{\textquotedblright} property, wherein function calls and returns are precisely matched in the abstract semantics, simply by virtue of the function call mechanism of the defining-language. 
{\par}
The first approaches to achieve this property for higher-order languages appeared within the last ten years, and have since been the subject of many papers. These approaches start from a state-machine semantics and uniformly involve significant technical engineering to recover the precision of pushdown control flow. In contrast, starting from a definitional interpreter, the pushdown control flow property is inherent in the meta-language and requires no further technical mechanism to achieve.},
}

@inproceedings{dvanhorn:VanHorn2010Abstracting,
  author = {Van Horn, David and Might, Matthew},
  title = {Abstracting abstract machines},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP~'10},
  location = {Baltimore, Maryland, USA},
  pages = {51--62},
  numpages = {12},
  month = sep,
  year = {2010},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-60558-794-3},
  doi = {10.1145/1863543.1863553},
  acmid = {1863553},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=1863543.1863553},
  keywords = {abstract interpretation; abstract machines},
  abstract = {We describe a derivational approach to abstract interpretation that yields novel and transparently sound static analyses when applied to well-established abstract machines. To demonstrate the technique and support our claim, we transform the CEK machine of Felleisen and Friedman, a lazy variant of Krivine's machine, and the stack-inspecting CM machine of Clements and Felleisen into abstract interpretations of themselves. The resulting analyses bound temporal ordering of program events; predict return-flow and stack-inspection behavior; and approximate the flow and evaluation of by-need parameters. For all of these machines, we find that a series of well-known concrete machine refactorings, plus a technique we call store-allocated continuations, leads to machines that abstract into static analyses simply by bounding their stores. We demonstrate that the technique scales up uniformly to allow static analysis of realistic language features, including tail calls, conditionals, side effects, exceptions, first-class continuations, and even garbage collection.},
}

@inproceedings{dvanhorn:nielson-nielson-popl97,
  author = {Nielson, Flemming and Nielson, Hanne Riis},
  title = {Infinitary control flow analysis: a collecting semantics for closure analysis},
  booktitle = {Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL~'97},
  location = {Paris, France},
  pages = {332--345},
  numpages = {14},
  month = jan,
  year = {1997},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {0-89791-853-3},
  doi = {10.1145/263699.263745},
  acmid = {263745},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=263699.263745},
  abstract = {Defining the collecting semantics is usually the first crucial step in adapting the general methodology of abstract interpretation to the semantic framework or programming language at hand. In this paper we show how to define a collecting semantics for control flow analysis: due to the generality of the formulation we need to appeal to coinduction (or greatest fixed points) in order to define the analysis. We then prove the semantic soundness of the collecting semantics and that all totally deterministic instantiations have a least solution; this incorporates \textit{k}-CFA, polymorphic splitting and a new class of uniform-\textit{k}-CFA analyses.},
}

@inproceedings{midtgaard2008calculational,
  author = {Midtgaard, Jan and Jensen, Thomas},
  editor = {Alpuente, Mar{\'i}a and Vidal, Germ{\'a}n},
  affiliation = {INRIA Rennes - Bretagne Atlantique and CNRS IRISA},
  title = {A Calculational Approach to Control-Flow Analysis by Abstract Interpretation},
  booktitle = {Static Analysis},
  volume = {5079},
  series = {Lecture Notes in Computer Science},
  pages = {347--362},
  year = {2008},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  language = {English},
  isbn = {978-3-540-69163-1 (Print) 978-3-540-69166-2 (Online)},
  issn = {0302-9743 (Print) 1611-3349 (Online)},
  doi = {10.1007/978-3-540-69166-2_23},
  bib_scrape_url = {https://link.springer.com/chapter/10.1007%2F978-3-540-69166-2_23},
  abstract = {We present a derivation of a control-flow analysis by abstract interpretation. Our starting point is a transition system semantics defined as an abstract machine for a small functional language in continuation-passing style. We obtain a Galois connection for abstracting the machine states by composing Galois connections, most notable an independent-attribute Galois connection on machine states and a Galois connection induced by a closure operator associated with a constituent-parts relation on environments. We calculate abstract transfer functions by applying the state abstraction to the collecting semantics, resulting in a novel characterization of demand-driven 0-CFA.},
}

@inproceedings{heintze2001demand,
  author = {Heintze, Nevin and Tardieu, Olivier},
  title = {Demand-driven pointer analysis},
  booktitle = {Proceedings of the ACM SIGPLAN 2001 Conference on Programming Language Design and Implementation},
  series = {PLDI~'01},
  location = {Snowbird, Utah, USA},
  pages = {24--34},
  numpages = {11},
  month = jun,
  year = {2001},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {1-58113-414-2},
  doi = {10.1145/378795.378802},
  acmid = {378802},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=378795.378802},
}

@inproceedings{sridharan2005demand,
  author = {Sridharan, Manu and Gopan, Denis and Shan, Lexin and Bod{\'{\i}}k, Rastislav},
  title = {Demand-driven points-to analysis for Java},
  booktitle = {Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA~'05},
  location = {San Diego, CA, USA},
  pages = {59--76},
  numpages = {18},
  month = oct,
  year = {2005},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {1-59593-031-0},
  doi = {10.1145/1094811.1094817},
  acmid = {1094817},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=1094811.1094817},
  keywords = {context-free language reachability; demand-driven analysis; points-to analysis; refinement},
  abstract = {We present a points-to analysis technique suitable for environments with small time and memory budgets, such as just-in-time (JIT) compilers and interactive development environments (IDEs). Our technique is demand-driven, performing only the work necessary to answer each query (a request for a variable's points-to information) issued by a client. In cases where even the demand-driven approach exceeds the time budget for a query, we employ \textit{early termination}, \textit{i.e.}, stopping the analysis prematurely and returning an over-approximated result to the client. Our technique improves on previous demand-driven points-to analysis algorithms [17, 33] by achieving much higher precision under small time budgets and early termination.We formulate Andersen's analysis [5] for Java as a CFL-reachability problem [33]. This formulation shows that Andersen's analysis for Java is a \textit{balanced-parentheses problem}, an insight that enables our new techniques. We exploit the balanced parentheses structure to approximate Andersen's analysis by \textit{regularizing} the CFL-reachability problem, yielding an asymptotically cheaper algorithm. We also show how to regain most of the precision lost in the regular approximation as needed through \textit{refinement}. Our evaluation shows that our regularization and refinement approach achieves nearly the precision of field-sensitive Andersen's analysis in time budgets as small as 2ms per query. Our technique can yield speedups of up to 16x over computing an exhaustive Andersen's analysis for some clients, with little to no precision loss.},
}

@inproceedings{dvanhorn:heintze-mcallester-pldi97,
  author = {Heintze, Nevin and McAllester, David},
  title = {Linear-time subtransitive control flow analysis},
  booktitle = {Proceedings of the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation},
  series = {PLDI~'97},
  location = {Las Vegas, Nevada, USA},
  pages = {261--272},
  numpages = {12},
  month = may,
  year = {1997},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {0-89791-907-6},
  doi = {10.1145/258915.258939},
  acmid = {258939},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=258915.258939},
  abstract = {We present a linear-time algorithm for bounded-type programs that builds a directed graph whose transitive closure gives exactly the results of the standard (cubic-time) Control-Flow Analysis (CFA) algorithm. Our algorithm can be used to list all functions calls from all call sites in (optimal) quadratic time. More importantly, it can be used to give linear-time algorithms for CFA-consuming applications such as:{\textbullet} effects analysis: find the side-effecting expressions in a program.{\textbullet} \textit{k}-limited CFA: for each call-site, list the functions if there are only a few of them ({\ensuremath{\leq}} k) and otherwise output {\textquotedbl}many{\textquotedbl}.{\textbullet} called-once analysis: identify all functions called from only one call-site.},
}

@inproceedings{sridharan2006refinement,
  author = {Sridharan, Manu and Bod{\'{\i}}k, Rastislav},
  title = {Refinement-based context-sensitive points-to analysis for Java},
  booktitle = {Proceedings of the 27th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI~'06},
  location = {Ottawa, Ontario, Canada},
  pages = {387--400},
  numpages = {14},
  month = jun,
  year = {2006},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {1-59593-320-4},
  doi = {10.1145/1133981.1134027},
  acmid = {1134027},
  bib_scrape_url = {http://doi.acm.org/10.1145/1133981.1134027},
  keywords = {context-sensitive analysis; demand-driven analysis; points-to analysis; refinement},
  abstract = {We present a scalable and precise context-sensitive points-to analysis with three key properties: (1) filtering out of unrealizable paths, (2) a context-sensitive heap abstraction, and (3) a context-sensitive call graph. Previous work [21] has shown that all three properties are important for precisely analyzing large programs, e.g., to show safety of downcasts. Existing analyses typically give up one or more of the properties for scalability. We have developed a refinement-based analysis that succeeds by simultaneously refining handling of method calls and heap accesses, allowing the analysis to precisely analyze important code while entirely skipping irrelevant code. The analysis is demanddriven and client-driven, facilitating refinement specific to each queried variable and increasing scalability. In our experimental evaluation, our analysis proved the safety of 61{\%} more casts than one of the most precise existing analyses across a suite of large benchmarks. The analysis checked the casts in under 13 minutes per benchmark (taking less than 1 second per query) and required only 35MB of memory, far less than previous approaches.},
}

@inproceedings{dvanhorn:heintze-mcallester-lics97,
    author = {Heintze, Nevin and Mcallester, David},
    title = {On the Cubic Bottleneck in Subtyping and Flow Analysis},
    booktitle = {Proceedings of the 12th Annual IEEE Symposium on Logic in Computer Science},
    series = {LICS '97},
    citeulike-article-id = {5394951},
    date-added = {2009-08-07 21:29:15},
    keywords = {file-import-09-08-07},
    priority = {2},
    publisher = {IEEE Computer Society},
    address = {Washington, DC, USA},
    pages = {342--351},
    year = {1997}
}

@inproceedings{dvanhorn:Might:2006:GammaCFA,
  author = {Might, Matthew and Shivers, Olin},
  title = {Improving flow analyses via {\textGamma}{CFA}: abstract garbage collection and counting},
  booktitle = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP~'06},
  location = {Portland, Oregon, USA},
  pages = {13--25},
  numpages = {13},
  month = sep,
  year = {2006},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {1-59593-309-3},
  doi = {10.1145/1159803.1159807},
  acmid = {1159807},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=1159803.1159807},
  keywords = {CPS; abstract counting; abstract garbage collection; continuations; environment analysis; flow analysis; functional languages; gamma-CFA; inlining; lambda calculus; program analysis; superbeta},
  abstract = {We present two independent and complementary improvements for flow-based analysis of higher-order languages: (1) \textit{abstract garbage collection} and (2) \textit{abstract counting}, collectively titled {\textgreek{G}}CFA.Abstract garbage collection is an analog to its concrete counterpart: we determine when an abstract resource has become unreachable, and then reallocate it as fresh. This prevents flow sets from merging in the abstract, which has two immediate effects: (1) the precision of the analysis is increased, and (2) the running time of the analysis is frequently reduced. In some nontrivial cases, we achieve an order of magnitude improvement in precision and time \textit{simultaneously}.In abstract counting, we track how many times an abstract resource has been allocated. A count of one implies that the abstract resource momentarily represents only one concrete resource. This, in turn, allows us to perform environment analysis and to expand the kinds (rather than just the degree) of optimizations available to the compiler.},
}

@inproceedings{biswas1997demand,
  author = {Biswas, Sandip K.},
  title = {A demand-driven set-based analysis},
  booktitle = {Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL~'97},
  location = {Paris, France},
  pages = {372--385},
  numpages = {14},
  month = jan,
  year = {1997},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {0-89791-853-3},
  doi = {10.1145/263699.263753},
  acmid = {263753},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?id=263753},
  abstract = {In this paper we present an analysis technique for isolating dead code in higher-order functional programs. First, we formalize what it means for a program fragment to contribute to the value returned by the program. Next, we provide a purely declarative specification of a subset of terms which constitute dead code. This is done by a refinement of the set-based semantics technique, developed by Nevin Heintze, by the introduction of a concept of demand. We then develop a demand-driven set-based analysis to compute dead code specified by the declarative specification. The demand-driven set-based analysis developed in this paper is polynomial time, in the size of the input program.},
}

@inproceedings{dvanhorn:VanHorn-Mairson:ICFP08,
  author = {Van{ }Horn, David and Mairson, Harry G.},
  title = {Deciding \textit{k}{CFA} is complete for {EXPTIME}},
  booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP~'08},
  location = {Victoria, BC, Canada},
  pages = {275--282},
  numpages = {8},
  month = sep,
  year = {2008},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-59593-919-7},
  doi = {10.1145/1411204.1411243},
  acmid = {1411243},
  bib_scrape_url = {http://portal.acm.org/citation.cfm?id=1411243},
  keywords = {complexity; flow analysis},
  abstract = {We give an exact characterization of the computational complexity of the \textit{k}CFA hierarchy. For any \textit{k} {\textgreater} 0, we prove that the control flow decision problem is complete for deterministic exponential time. This theorem validates empirical observations that such control flow analysis is intractable. It also provides more general insight into the complexity of abstract interpretation.},
}

@article{local:felleisen-TCS1992,
  author = {Felleisen, Matthias and Hieb, Robert},
  title = {The revised report on the syntactic theories of sequential control and state},
  journal = {Theoretical Computer Science},
  volume = {103},
  number = {2},
  pages = {235--271},
  month = sep,
  year = {1992},
  publisher = {Elsevier},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(92)90014-7},
  bib_scrape_url = {http://dx.doi.org/10.1016/0304-3975(92)90014-7},
  abstract = {The syntactic theories of control and state are conservative extensions of the {\textgreek{l}}\textsubscript{{\textgreek{u}}}-calculus for equational reasoning about imperative programming facilities in higher-order languages. Unlike the simple {\textgreek{l}}\textsubscript{{\textgreek{u}}}-calculus, the extended theories are mixtures of equivalence relations and compatible congruence relations on the term language, which significantly complicates the reasoning process. In this paper we develop fully compatible equational theories of the same imperative higher-order programming languages. The new theories subsume the original calculi of control and state and satisfy the usual Church{--}Rosser and Standardization Theorems. With the new calculi, equational reasoning about imperative programs becomes as simple as reasoning about functional programs.},
}

@inproceedings{local:might-popl07,
  author = {Might, Matthew},
  title = {Logic-flow analysis of higher-order programs},
  booktitle = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL~'07},
  location = {Nice, France},
  pages = {185--198},
  numpages = {14},
  month = jan,
  year = {2007},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {1-59593-575-4},
  doi = {10.1145/1190216.1190247},
  acmid = {1190247},
  bib_scrape_url = {http://doi.acm.org/10.1145/1190216.1190247},
  keywords = {CPS; LFA; abstract counting; abstract garbage collection; environment analysis; gamma-CFA first-order logic; lambda calculus; logic-flow analysis; static analysis; theorem proving},
  abstract = {This work presents a framework for fusing flow analysis and theorem proving called \textit{logic-flow analysis} (LFA). The framework itself is the reduced product of two abstract interpretations: (1) an abstract state machine and (2) a set of propositions in a restricted first-order logic. The motivating application for LFA is the safe removal of implicit array-bounds checks without type information, user interaction or program annotation. LFA achieves this by delegating a given task to either the prover or the flow analysis depending on which is best suited to discharge it. Described within are a concrete semantics for continuation-passing style; a restricted, first-order logic; a woven product of two abstract interpretations; proofs of correctness; and a worked example.},
}

@inproceedings{local:darais-oopsla2015,
  author = {Darais, David and Might, Matthew and Van Horn, David},
  title = {Galois transformers and modular abstract interpreters: reusable metatheory for program analysis},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA 2015},
  location = {Pittsburgh, PA, USA},
  pages = {552--571},
  numpages = {20},
  month = oct,
  year = {2015},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-3689-5},
  doi = {10.1145/2814270.2814308},
  acmid = {2814308},
  bib_scrape_url = {http://doi.acm.org/10.1145/2814270.2814308},
  keywords = {Galois connections; abstract interpretation; monads; program analysis},
  abstract = {The design and implementation of static analyzers has become increasingly systematic. Yet for a given language or analysis feature, it often requires tedious and error prone work to implement an analyzer and prove it sound. In short, static analysis features and their proofs of soundness do not compose well, causing a dearth of reuse in both implementation and metatheory. We solve the problem of systematically constructing static analyzers by introducing Galois transformers: monad transformers that transport Galois connection properties. In concert with a monadic interpreter, we define a library of monad transformers that implement building blocks for classic analysis parameters like context, path, and heap (in)sensitivity. Moreover, these can be composed together independent of the language being analyzed. Significantly, a Galois transformer can be proved sound once and for all, making it a reusable analysis component. As new analysis features and abstractions are developed and mixed in, soundness proofs need not be reconstructed, as the composition of a monad transformer stack is sound by virtue of its constituents. Galois transformers provide a viable foundation for reusable and composable metatheory for program analysis. Finally, these Galois transformers shift the level of abstraction in analysis design and implementation to a level where non-specialists have the ability to synthesize sound analyzers over a number of parameters.},
}

@inproceedings{local:steele-popl94,
  author = {Steele, Jr., Guy L.},
  title = {Building interpreters by composing monads},
  booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL~'94},
  location = {Portland, Oregon, USA},
  pages = {472--492},
  numpages = {21},
  month = feb,
  year = {1994},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {0-89791-636-0},
  doi = {10.1145/174675.178068},
  acmid = {178068},
  bib_scrape_url = {http://doi.acm.org/10.1145/174675.178068},
}

@inproceedings{local:p4f,
  author = {Gilray, Thomas and Lyde, Steven and Adams, Michael D. and Might, Matthew and Van Horn, David},
  title = {Pushdown control-flow analysis for free},
  booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL~'16},
  location = {St. Petersburg, FL, USA},
  pages = {691--704},
  numpages = {14},
  month = jan,
  year = {2016},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-3549-2},
  doi = {10.1145/2837614.2837631},
  acmid = {2837631},
  bib_scrape_url = {http://doi.acm.org/10.1145/2837614.2837631},
  keywords = {Abstract interpretation; Control-flow analysis; Pushdown analysis; Static analysis; Store-allocated continuations},
  abstract = {Traditional control-flow analysis (CFA) for higher-order languages introduces spurious connections between callers and callees, and different invocations of a function may pollute each other's return flows. Recently, three distinct approaches have been published that provide perfect call-stack precision in a computable manner: CFA2, PDCFA, and AAC. Unfortunately, implementing CFA2 and PDCFA requires significant engineering effort. Furthermore, all three are computationally expensive. For a monovariant analysis, CFA2 is in O(2^n), PDCFA is in O(n^6), and AAC is in O(n^8). In this paper, we describe a new technique that builds on these but is both straightforward to implement and computationally inexpensive. The crucial insight is an unusual state-dependent allocation strategy for the addresses of continuations. Our technique imposes only a constant-factor overhead on the underlying analysis and costs only O(n^3) in the monovariant case. We present the intuitions behind this development, benchmarks demonstrating its efficacy, and a proof of the precision of this analysis.},
}

@inproceedings{local:flatt-pldi98,
  author = {Flatt, Matthew and Felleisen, Matthias},
  title = {Units: cool modules for {HOT} languages},
  booktitle = {Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation},
  series = {PLDI~'98},
  location = {Montreal, Quebec, Canada},
  pages = {236--248},
  numpages = {13},
  month = may,
  year = {1998},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {0-89791-987-4},
  doi = {10.1145/277650.277730},
  acmid = {277730},
  bib_scrape_url = {http://doi.acm.org/10.1145/277650.277730},
  abstract = {A module system ought to enable \textit{assembly-line programming} using separate compilation and an expressive linking language. Separate compilation allows programmers to develop parts of a program independently. A linking language gives programmers precise control over the assembly of parts into a whole. This paper presents models of \textit{program units}, MzScheme's module language for assembly-line programming. Units support separate compilation, independent module reuse, cyclic dependencies, hierarchical structuring, and dynamic linking. The models explain how to integrate units with untyped and typed languages such as Scheme and ML.},
}

@inproceedings{local:rtac,
  author = {Malecha, Gregory and Bengtson, Jesper},
  editor = {Thiemann, Peter},
  affiliation = {University of California and IT University of Copenhagen},
  title = {Extensible and Efficient Automation Through Reflective Tactics},
  booktitle = {Programming Languages and Systems},
  volume = {9632},
  series = {Lecture Notes in Computer Science},
  pages = {532--559},
  year = {2016},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  language = {English},
  isbn = {978-3-662-49497-4 (Print) 978-3-662-49498-1 (Online)},
  issn = {0302-9743 (Print) 1611-3349 (Online)},
  doi = {10.1007/978-3-662-49498-1_21},
  bib_scrape_url = {http://dx.doi.org/10.1007/978-3-662-49498-1_21},
  abstract = {Foundational proof assistants simultaneously offer both expressive logics and strong guarantees. The price they pay for this flexibility is often the need to build and check explicit proof objects which can be expensive. In this work we develop a collection of techniques for building reflective automation, where proofs are witnessed by verified decision procedures rather than verbose proof objects. Our techniques center around a verified domain specific language for proving, \(\mathcal {R}_{tac}\), written in Gallina, Coq{\textquoteright}s logic. The design of tactics makes it easy to combine them into higher-level automation that can be proved sound in a mostly automated way. Furthermore, unlike traditional uses of reflection, \(\mathcal {R}_{tac}\) tactics are independent of the underlying problem domain, which allows them to be re-tasked to automate new problems with very little effort. We demonstrate the usability of \(\mathcal {R}_{tac}\) through several case studies demonstrating orders of magnitude speedups for relatively little engineering work.},
}

@article{dvanhorn:ashley-dybvig-toplas98,
  author = {Ashley, J. Michael and Dybvig, R. Kent},
  title = {A practical and flexible flow analysis for higher-order languages},
  journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume = {20},
  number = {4},
  pages = {845--868},
  numpages = {24},
  month = jul,
  year = {1998},
  issue_date = {July 1998},
  publisher = {ACM},
  address = {New York, NY, USA},
  issn = {0164-0925},
  doi = {10.1145/291891.291898},
  acmid = {291898},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=291891.291898},
  keywords = {abstract interpretation; higher-order languages},
  abstract = {A flow analysis collects data-flow and control-flow information about programs. A compiler can use this information to enable optimizations. The analysis described in this article unifies and extends previous work on flow analysis for higher-order languages supporting assignment and control operators. The analysis is abstract interpretation based and is parameterized over two polyvariance operators and a projection operator. These operators are used to regulate the speed and accuracy of the analysis. An implementation of the analysis is incorporated into and used in a production Scheme compiler. The analysis can process any legal Scheme program without modification. Others have demonstrated that a 0CFA analysis can enables the optimizations, but a 0CFA analysis is \textit{O}(\textit{n})\textsuperscript{3}). An \textit{O}(\textit{n}) instantiation of our analysis successfully enables the optimization of closure representations and procedure calls. Experiments with the cheaper instantiation show that it is as effective as 0CFA for these optimizations.},
}

@inproceedings{ashley1996,
  author = {Ashley, J. Michael},
  title = {A practical and flexible flow analysis for higher-order languages},
  booktitle = {Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL~'96},
  location = {St. Petersburg Beach, Florida, USA},
  pages = {184--194},
  numpages = {11},
  month = jan,
  year = {1996},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {0-89791-769-3},
  doi = {10.1145/237721.237773},
  acmid = {237773},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=237721.237773},
  abstract = {A flow analysis framework for higher-order, mostly-functional languages is given. The framework unifies and extends previous work on flow analyses for this class of programming languages. The analysis is based on abstract interpretation and is parameterized over the abstraction of literals, two polyvariance operators, and a projection operator. The polyvariance operators regulate the accuracy of the analysis while the projection operator regulates the speed. A preliminary implementation of the analysis is incorporated and used in a production-quality Scheme compiler. The analysis can process any legal Scheme program without modification. While it has been demonstrated that analyses at least as accurate as OCFA are useful for justifying program transformations, an instantiation of this analysis less precise than OCFA is used to facilitate loop recognition, eliminate the construction of closures, and optimize procedure calls. This demonstrates that relatively inaccurate analyses can still be useful for justifying transformations.},
}

@inproceedings{dvanhorn:Might2010Resolving,
  author = {Might, Matthew and Smaragdakis, Yannis and Van Horn, David},
  title = {Resolving and exploiting the \textit{k}-{CFA} paradox: illuminating functional vs. object-oriented program analysis},
  booktitle = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI~'10},
  location = {Toronto, Ontario, Canada},
  pages = {305--315},
  numpages = {11},
  month = jun,
  year = {2010},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-0019-3},
  doi = {10.1145/1806596.1806631},
  acmid = {1806631},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=1806596.1806631},
  keywords = {control-flow analysis; functional; k-cfa; m-cfa; object-oriented; pointer analysis; static analysis},
  abstract = {Low-level program analysis is a fundamental problem, taking the shape of {\textquotedbl}flow analysis{\textquotedbl} in functional languages and {\textquotedbl}points-to{\textquotedbl} analysis in imperative and object-oriented languages. Despite the similarities, the vocabulary and results in the two communities remain largely distinct, with limited cross-understanding. One of the few links is Shivers's \textit{k}-CFA work, which has advanced the concept of {\textquotedbl}context-sensitive analysis{\textquotedbl} and is widely known in both communities.
{\par}
Recent results indicate that the relationship between the functional and object-oriented incarnations of \textit{k}-CFA is not as well understood as thought. Van Horn and Mairson proved \textit{k}-CFA for \textit{k} {\ensuremath{\geq}} 1 to be EXPTIME-complete; hence, no polynomial-time algorithm can exist. Yet, there are several polynomial-time formulations of context-sensitive points-to analyses in object-oriented languages. Thus, it seems that functional \textit{k}-CFA may actually be a profoundly different analysis from object-oriented \textit{k}-CFA. We resolve this paradox by showing that the exact same specification of \textit{k}-CFA is polynomial-time for object-oriented languages yet exponential-time for functional ones: objects and closures are subtly different, in a way that interacts crucially with context-sensitivity and complexity. This illumination leads to an immediate payoff: by projecting the object-oriented treatment of objects onto closures, we derive a polynomial-time hierarchy of context-sensitive CFAs for functional programs.},
}

@inproceedings{germane2019demand,
  author = {Germane, Kimball and McCarthy, Jay and Adams, Michael D. and Might, Matthew},
  editor = {Enea, Constantin and Piskac, Ruzica},
  affiliation = {Brigham Young University and University of Massachusetts Lowell and University of Utah and University of Alabama},
  title = {Demand Control-Flow Analysis},
  booktitle = {Verification, Model Checking, and Abstract Interpretation},
  volume = {11388},
  series = {Lecture Notes in Computer Science},
  pages = {226--246},
  year = {2019},
  publisher = {Springer},
  address = {Cham},
  language = {English},
  isbn = {978-3-030-11244-8 (Print) 978-3-030-11245-5 (Online)},
  issn = {0302-9743 (Print) 1611-3349 (Online)},
  doi = {10.1007/978-3-030-11245-5_11},
  bib_scrape_url = {https://link.springer.com/chapter/10.1007%2F978-3-030-11245-5_11},
  abstract = {Points-to analysis manifests in a functional setting as control-flow analysis. Despite the ubiquity of \emph{demand} points-to analyses, there are no analogous demand control-flow analyses for functional languages in general. We present demand 0CFA, a demand control-flow analysis that offers clients in a functional setting the same pricing model that demand points-to analysis clients enjoy in an imperative setting. We establish demand 0CFA{\textquoteright}s correctness via an intermediary exact semantics, demand evaluation, that can potentially support demand variants of more-precise analyses.},
}

@article{duesterwald1997practical,
  author = {Duesterwald, Evelyn and Gupta, Rajiv and Soffa, Mary Lou},
  title = {A practical framework for demand-driven interprocedural data flow analysis},
  journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume = {19},
  number = {6},
  pages = {992--1030},
  numpages = {39},
  month = nov,
  year = {1997},
  issue_date = {Nov. 1997},
  publisher = {ACM},
  address = {New York, NY, USA},
  issn = {0164-0925},
  doi = {10.1145/267959.269970},
  acmid = {269970},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=267959.269970},
  keywords = {copy constant propagation; data flow analysis; def-use chains; demand-driven algorithms; distributive data flow frameworks; interprocedural data flow analysis; program optimizations},
  abstract = {The high cost and growing importance of interprocedural data flow analysis have led to an increased interest in demand-driven algorithms. In this article, we present a general framework for developing demand-driven interprocedural data flow analyzers and report our experience in evaluating the performance of this approach. A demand for data flow information is modeled as a set of queries. The framework includes a generic demand-driven algorithm that determines the response to query by iteratively applying a system of query propagation rules. The propagation rules yield precise responses for the class of distributive finite data flow problems. We also describe a two-phase framework variation to accurately handle nondistributive problems. A performance evaluation of our demand-driven approach is presented for two data flow problems, namely, reaching-definitions and copy constant propagation. Our experiments show that demand-driven analysis performs well in practice, reducing both time and space requirements when compared with exhaustive analysis.},
}

@article{debruijn,
  author = {de Bruijn, N[icolaas] G[overt]},
  title = {Lambda calculus notation with nameless dummies, a tool for automatic formula manipulation, with application to the Church-Rosser theorem},
  journal = {Indagationes Mathematicae (Proceedings)},
  volume = {75},
  number = {5},
  pages = {381--392},
  month = jan,
  year = {1972},
  publisher = {North-Holland},
  issn = {1385-7258},
  doi = {10.1016/1385-7258(72)90034-0},
  bib_scrape_url = {https://www.sciencedirect.com/science/article/pii/1385725872900340},
  abstract = {In ordinary lambda calculus the occurrences of a bound variable are made recognizable by the use of one and the same (otherwise irrelevant) name at all occurrences. This convention is known to cause considerable trouble in cases of substitution. In the present paper a different notational system is developed, where occurrences of variables are indicated by integers giving the {\textquotedblleft}distance{\textquotedblright} to the binding {\textgreek{l}} instead of a name attached to that {\textgreek{l}}. The system is claimed to be efficient for automatic formula manipulation as well as for metalingual discussion. As an example the most essential part of a proof of the Church-Rosser theorem is presented in this namefree calculus.},
}

@inproceedings{dvanhorn:Smaragdakis2011Pick,
  author = {Smaragdakis, Yannis and Bravenboer, Martin and Lhot{\'a}k, Ondrej},
  title = {Pick your contexts well: understanding object-sensitivity},
  booktitle = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL~'11},
  location = {Austin, Texas, USA},
  pages = {17--30},
  numpages = {14},
  month = jan,
  year = {2011},
  publisher = {ACM},
  address = {New York, NY, USA},
  isbn = {978-1-4503-0490-0},
  doi = {10.1145/1926385.1926390},
  acmid = {1926390},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=1925844.1926390},
  keywords = {context-sensitivity; object-sensitivity; points-to analysis; type-sensitivity},
  abstract = {Object-sensitivity has emerged as an excellent context abstraction for points-to analysis in object-oriented languages. Despite its practical success, however, object-sensitivity is poorly understood. For instance, for a context depth of 2 or higher, past scalable implementations deviate significantly from the original definition of an object-sensitive analysis. The reason is that the analysis has many degrees of freedom, relating to which context elements are picked at every method call and object creation. We offer a clean model for the analysis design space, and discuss a formal and informal understanding of object-sensitivity and of how to create good object-sensitive analyses. The results are surprising in their extent. We find that past implementations have made a sub-optimal choice of contexts, to the severe detriment of precision and performance. We define a {\textquotedbl}full-object-sensitive{\textquotedbl} analysis that results in significantly higher precision, and often performance, for the exact same context depth. We also introduce {\textquotedbl}type-sensitivity{\textquotedbl} as an explicit approximation of object-sensitivity that preserves high context quality at substantially reduced cost. A type-sensitive points-to analysis makes an unconventional use of types as context: the context types are not dynamic types of objects involved in the analysis, but instead upper bounds on the dynamic types of their allocator objects. Our results expose the influence of context choice on the quality of points-to analysis and demonstrate type-sensitivity to be an idea with major impact: It decisively advances the state-of-the-art with a spectrum of analyses that simultaneously enjoy speed (several times faster than an analogous object-sensitive analysis), scalability (comparable to analyses with much less context-sensitivity), and precision (comparable to the best object-sensitive analysis with the same context depth).},
}

@book{dvanhorn:Sperber2010Revised,
  author = {Sperber, Michael and Dybvig, R. Kent and Flatt, Matthew and van Straaten, Anton and Findler, Robby and Matthews, Jacob},
  title = {Revised [6] Report on the Algorithmic Language {S}cheme},
  edition = {1st},
  month = jun,
  year = {2010},
  publisher = {Cambridge University Press},
  address = {New York, NY, USA},
  isbn = {0-521-19399-0},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?id=1830448},
  abstract = {Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary. Scheme demonstrates that a very small number of rules for forming expressions, with no restrictions on how they are composed, are enough to form a practical and efficient programming language that is flexible enough to support most of the major programming paradigms in use today. This book contains the three parts comprising 'R6RS', the sixth revision of a series of reports describing the programming language Scheme. The book is divided into parts: a description of the language itself, a description of the standard libraries and non-normative appendices. Early chapters introduce Scheme and later chapters act as a reference manual. This is an important report for programmers that work with or want to learn about the Scheme language.},
}

@article{Brzozowski:1964,
  author = {Brzozowski, Janusz A.},
  title = {Derivatives of Regular Expressions},
  journal = {Journal of the ACM (JACM)},
  volume = {11},
  number = {4},
  pages = {481--494},
  numpages = {14},
  month = oct,
  year = {1964},
  issue_date = {Oct. 1964},
  publisher = {ACM},
  address = {New York, NY, USA},
  issn = {0004-5411},
  doi = {10.1145/321239.321249},
  acmid = {321249},
  bib_scrape_url = {https://dl.acm.org/citation.cfm?doid=321239.321249},
}
